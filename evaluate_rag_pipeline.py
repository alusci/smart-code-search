import os
from utils.models import initialize_llm
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall
)
from ragas.run_config import RunConfig
from ragas import evaluate
from datasets import Dataset
import json


def evaluate_rag_pipeline(
    queries,
    retrieved_contexts,
    generated_answers,
    expected_answers,
    openai_api_key=None
):
    """
    Evaluate the RAG pipeline using RAGAS metrics.
    
    Args:
        queries (list): List of question strings
        retrieved_contexts (list): List of lists, each containing retrieved context strings for each query
        generated_answers (list): List of answer strings generated by the RAG pipeline
        expected_answers (list): List of expected (ground truth) answer strings
        openai_api_key (str, optional): OpenAI API key. Defaults to environment variable.
    
    Returns:
        pandas.DataFrame: Evaluation results
    """
    # Use environment variable if not provided
    if not openai_api_key:
        openai_api_key = os.environ.get("OPENAI_API_KEY")
    
    if not openai_api_key:
        raise ValueError("OpenAI API key not found. Please provide it explicitly or set it as an environment variable.")
    
    # Define the LLM to use for evaluation
    llm = initialize_llm()
    
    # Define metrics to evaluate
    metrics = [
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall
    ]
    
    # Convert to Dataset format required by RAGAS
    eval_dataset = Dataset.from_dict({
        "question": queries,
        "contexts": retrieved_contexts,
        "answer": generated_answers,
        "ground_truth": expected_answers
    })
    
    # Run evaluation
    results = evaluate(
        dataset=eval_dataset,
        metrics=metrics,
        llm=llm,
        run_config=RunConfig(max_workers=2, timeout=60)
    )
    
    return results.to_pandas()

def parse_evaluation_questions(questions):
    """
    Parse evaluation questions from a JSON file.
    
    Args:
        questions (list): List of question dictionaries
    
    Returns:
        list: List of parsed question strings
    """

    evaluation_data = {
        "questions": [],
        "answers": [],
        "rag_answers": [],
        "rag_contexts": []
    }

    evaluation_questions, retrieved_contexts, generated_answers, expected_answers = [], [], [], []
    for question in questions:
        evaluation_data["questions"].append(question["question"])
        evaluation_data["answers"].append(question["answer"])
        evaluation_data["rag_answers"].append(question["rag_answer"])
        evaluation_data["rag_contexts"].append(question["rag_context"])

    return evaluation_data
    
        
       
if __name__ == "__main__":

    # Load evaluation questions and answers
    with open("data/evaluation_questions.json", "r") as fp:
        evaluation_questions = json.load(fp)

    evaluation_data = parse_evaluation_questions(evaluation_questions)
    
    try:
        # Run evaluation
        results = evaluate_rag_pipeline(
            queries=evaluation_data["questions"],
            retrieved_contexts=evaluation_data["rag_contexts"],
            generated_answers=evaluation_data["rag_answers"],
            expected_answers=evaluation_data["answers"],
        )
        
        print(results.head())
        print("Evaluation results:")
        metrics = [
            'faithfulness', 'answer_relevancy', 'context_precision', 'context_recall'
        ]
        for metric in metrics:
            print(f"{metric}: {results[metric].mean():.4f}")

        # Save results to CSV
        os.makedirs("data", exist_ok=True)
        results.to_csv("data/rag_evaluation_results.csv", index=False)
            
    except Exception as e:
        print(f"Error during evaluation: {e}")
        import traceback
        traceback.print_exc()