import os
import argparse
from utils.models import initialize_llm
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall
)
from ragas.run_config import RunConfig
from ragas import evaluate
from datasets import Dataset
import json


def evaluate_rag_pipeline(
    queries,
    retrieved_contexts,
    generated_answers,
    expected_answers,
    openai_api_key=None
):
    """
    Evaluate the RAG pipeline using RAGAS metrics.
    
    Args:
        queries (list): List of question strings
        retrieved_contexts (list): List of lists, each containing retrieved context strings for each query
        generated_answers (list): List of answer strings generated by the RAG pipeline
        expected_answers (list): List of expected (ground truth) answer strings
        openai_api_key (str, optional): OpenAI API key. Defaults to environment variable.
    
    Returns:
        pandas.DataFrame: Evaluation results
    """
    # Use environment variable if not provided
    if not openai_api_key:
        openai_api_key = os.environ.get("OPENAI_API_KEY")
    
    if not openai_api_key:
        raise ValueError("OpenAI API key not found. Please provide it explicitly or set it as an environment variable.")
    
    # Define the LLM to use for evaluation
    llm = initialize_llm()
    
    # Define metrics to evaluate
    metrics = [
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall
    ]
    
    # Convert to Dataset format required by RAGAS
    eval_dataset = Dataset.from_dict({
        "question": queries,
        "contexts": retrieved_contexts,
        "answer": generated_answers,
        "ground_truth": expected_answers
    })
    
    # Run evaluation
    results = evaluate(
        dataset=eval_dataset,
        metrics=metrics,
        llm=llm,
        run_config=RunConfig(max_workers=2, timeout=60)
    )
    
    return results.to_pandas()

def parse_evaluation_questions(questions):
    """
    Parse evaluation questions from a JSON file.
    
    Args:
        questions (list): List of question dictionaries
    
    Returns:
        list: List of parsed question strings
    """

    evaluation_data = {
        "questions": [],
        "answers": [],
        "rag_answers": [],
        "rag_contexts": []
    }

    evaluation_questions, retrieved_contexts, generated_answers, expected_answers = [], [], [], []
    for question in questions:
        evaluation_data["questions"].append(question["question"])
        evaluation_data["answers"].append(question["answer"])
        evaluation_data["rag_answers"].append(question["rag_answer"])
        evaluation_data["rag_contexts"].append(question["rag_context"])

    return evaluation_data
    

def parse_arguments():
    """
    Parse command line arguments.
    """
    parser = argparse.ArgumentParser(description="Evaluate RAG pipeline performance")
    
    parser.add_argument(
        "--input-file", 
        "-i",
        type=str, 
        default="data/evaluation_questions.json",
        help="Path to the evaluation questions JSON file"
    )
    
    parser.add_argument(
        "--output-file",
        "-o", 
        type=str, 
        default="data/rag_evaluation_results.csv",
        help="Path to save the evaluation results CSV file"
    )
    
    parser.add_argument(
        "--workers",
        "-w",
        type=int,
        default=2,
        help="Number of workers for parallel evaluation"
    )
    
    parser.add_argument(
        "--timeout",
        "-t",
        type=int,
        default=60,
        help="Timeout in seconds for evaluation requests"
    )
    
    return parser.parse_args()
       
       
if __name__ == "__main__":
    # Parse command line arguments
    args = parse_arguments()
    
    # Create output directory if it doesn't exist
    os.makedirs(os.path.dirname(args.output_file), exist_ok=True)

    # Load evaluation questions and answers
    print(f"Loading evaluation data from {args.input_file}")
    try:
        with open(args.input_file, "r") as fp:
            evaluation_questions = json.load(fp)
            all_questions = len(evaluation_questions)
            invalid_answer = "I don't have enough information to answer this question."
            evaluation_questions = [
                question for question in evaluation_questions if question["rag_answer"] != invalid_answer
            ]
            print("Found {} valid questions out of {}".format(len(evaluation_questions), all_questions))
    except FileNotFoundError:
        print(f"Error: Input file '{args.input_file}' not found.")
        exit(1)
    except json.JSONDecodeError:
        print(f"Error: Input file '{args.input_file}' is not a valid JSON file.")
        exit(1)

    evaluation_data = parse_evaluation_questions(evaluation_questions)
    print(f"Loaded {len(evaluation_data['questions'])} evaluation questions")
    
    try:
        print("Running evaluation...")
        # Run evaluation
        results = evaluate_rag_pipeline(
            queries=evaluation_data["questions"],
            retrieved_contexts=evaluation_data["rag_contexts"],
            generated_answers=evaluation_data["rag_answers"],
            expected_answers=evaluation_data["answers"],
        )
        
        print(results.head())
        print("\nEvaluation results:")
        metrics = [
            'faithfulness', 'answer_relevancy', 'context_precision', 'context_recall'
        ]
        for metric in metrics:
            print(f"{metric}: {results[metric].mean():.4f}")

        # Save results to CSV
        results.to_csv(args.output_file, index=False)
        print(f"Results saved to {args.output_file}")
            
    except Exception as e:
        print(f"Error during evaluation: {e}")
        import traceback
        traceback.print_exc()